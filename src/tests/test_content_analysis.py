#!/usr/bin/env python3
"""
Test script for the content analysis and filtering system

This script:
1. Tests text normalization
2. Tests location matching
3. Tests price and room filtering
4. Tests match scoring and ranking
5. Demonstrates complete analysis flow

Run with: python scripts/test_content_analysis.py
"""

import sys
from pathlib import Path

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from scrapers import ScrapedListing
from analysis import ContentAnalyzer, MatchResult, MatchConfidence

def create_test_listings():
    """Create sample listings for testing"""
    return [
        ScrapedListing(
            listing_id="test_1",
            title="×“×™×¨×ª 2 ×—×“×¨×™× ×‘×“×™×–× ×’×•×£ - ××¨×•×”×˜×ª ×•××©×•×¤×¦×ª",
            price=5800,
            rooms=2.0,
            location="×“×™×–× ×’×•×£ 45, ×ª×œ ××‘×™×‘ - ×™×¤×•",
            description="×“×™×¨×” ×™×¤×” ×•××¨×•×”×˜×ª ×‘××¨×›×– ×“×™×–× ×’×•×£. ××™×–×•×’ ××•×•×™×¨, ××¨×¤×¡×ª, ×§×¨×•×‘ ×œ×ª×—×‘×•×¨×” ×¦×™×‘×•×¨×™×ª.",
            features=["××¨×•×”×˜×ª", "××™×–×•×’ ××•×•×™×¨", "××¨×¤×¡×ª"]
        ),
        ScrapedListing(
            listing_id="test_2", 
            title="×¡×˜×•×“×™×• ×‘×¨×•×˜×©×™×œ×“ - ×–××™×Ÿ ××™×“",
            price=4200,
            rooms=1.0,
            location="×¨×•×˜×©×™×œ×“ 88, ×ª×œ ××‘×™×‘ - ×™×¤×•",
            description="×¡×˜×•×“×™×• ×—×“×© ×•××¢×•×¦×‘ ×‘×¨×—×•×‘ ×¨×•×˜×©×™×œ×“. ×›×œ ×”×¦×™×•×“ ×›×œ×•×œ, ×—× ×™×”.",
            features=["×—×“×©", "××¢×•×¦×‘", "×—× ×™×”"]
        ),
        ScrapedListing(
            listing_id="test_3",
            title="3 ×—×“×¨×™× ×‘×¤×œ×•×¨× ×˜×™×Ÿ ×¢× ××¢×œ×™×ª",
            price=7200,
            rooms=3.0,
            location="×¤×œ×•×¨× ×˜×™×Ÿ 12, ×ª×œ ××‘×™×‘",
            description="×“×™×¨×” ×’×“×•×œ×” ×•××•××¨×ª ×‘×¤×œ×•×¨× ×˜×™×Ÿ. ××¢×œ×™×ª, ××¨×¤×¡×ª ×’×“×•×œ×”, ×©×§×˜.",
            features=["××¢×œ×™×ª", "××¨×¤×¡×ª", "×©×§×˜"]
        ),
        ScrapedListing(
            listing_id="test_4",
            title="×“×™×¨×ª 4 ×—×“×¨×™× ×‘× ×ª× ×™×” - ×™×§×¨×” ××“×™",
            price=9500,  # Too expensive
            rooms=4.0,
            location="× ×ª× ×™×” ××¨×›×–",
            description="×“×™×¨×” ×’×“×•×œ×” ×‘× ×ª× ×™×” ×¢× × ×•×£ ×œ×™×.",
            features=["× ×•×£ ×œ×™×"]
        ),
        ScrapedListing(
            listing_id="test_5",
            title="×—×“×¨ ×‘×©×•×ª×¤×•×ª ×‘×¨××ª ×’×Ÿ",
            price=2800,
            rooms=1.0,  # Single room
            location="×¨××ª ×’×Ÿ, ×œ×™×“ ×”×¨×›×‘×ª",
            description="×—×“×¨ ×‘×“×™×¨×ª ×©×•×ª×¤×•×ª ×¢× ×¡×˜×•×“× ×˜×™×.",
            features=["×©×•×ª×¤×•×ª"]
        )
    ]

def create_test_profile():
    """Create sample user profile criteria"""
    return {
        'price': {'min': 4000, 'max': 6500},
        'rooms': {'min': 1.0, 'max': 2.5},
        'location_criteria': {
            'city': '×ª×œ ××‘×™×‘ - ×™×¤×•',
            'neighborhoods': ['×“×™×–× ×’×•×£', '×¨×•×˜×©×™×œ×“', '×¤×œ×•×¨× ×˜×™×Ÿ'],
            'streets': ['×“×™×–× ×’×•×£', '×¨×•×˜×©×™×œ×“']
        },
        'property_type': ['×“×™×¨×”', '×¡×˜×•×“×™×•'],
        'preferred_features': ['××¨×¤×¡×ª', '××™×–×•×’', '×—× ×™×”']
    }

def test_text_normalization():
    """Test text normalization functionality"""
    print("ğŸ“ Testing Text Normalization")
    print("=" * 40)
    
    analyzer = ContentAnalyzer()
    
    test_texts = [
        "×“×™×¨×ª 2 ×—×“×¨×™× ××¨×•×”×˜×ª ×¢× ××¨×¤×¡×ª",
        "Apartment with 2 rooms and balcony",
        "×¡×˜×•×“×™×• ××©×•×¤×¥ ×‘×¨×•×˜×©×™×œ×“ ×¢× ××™×–×•×’ ××•×•×™×¨!!!",
        "×“×™×¨×”    ×¢×   ×¨×•×•×—×™×   ××™×•×ª×¨×™×"
    ]
    
    for text in test_texts:
        normalized = analyzer.normalize_text(text)
        print(f"Original: {text}")
        print(f"Normalized: {normalized}")
        print()
    
    return True

def test_individual_analysis():
    """Test analysis of individual listings"""
    print("\nğŸ” Testing Individual Listing Analysis")
    print("=" * 40)
    
    analyzer = ContentAnalyzer()
    profile = create_test_profile()
    listings = create_test_listings()
    
    print(f"Profile criteria:")
    print(f"  Price: {profile['price']['min']:,}-{profile['price']['max']:,} ILS")
    print(f"  Rooms: {profile['rooms']['min']}-{profile['rooms']['max']}")
    print(f"  City: {profile['location_criteria']['city']}")
    print(f"  Neighborhoods: {', '.join(profile['location_criteria']['neighborhoods'])}")
    print()
    
    for listing in listings:
        print(f"ğŸ“‹ Analyzing: {listing.title}")
        print(f"   ğŸ’° {listing.price:,} ILS | ğŸ  {listing.rooms} rooms | ğŸ“ {listing.location}")
        
        result = analyzer.analyze_listing(listing, profile)
        
        match_icon = "âœ…" if result.is_match else "âŒ"
        print(f"   {match_icon} Match: {result.is_match} | Confidence: {result.confidence.value} | Score: {result.score:.1f}")
        
        if result.location_matches:
            print(f"   ğŸ¯ Location matches: {', '.join(result.location_matches)}")
        
        if result.keyword_matches:
            print(f"   ğŸ”‘ Keywords: {', '.join(result.keyword_matches)}")
        
        # Show top reasons
        for reason in result.reasons[:3]:
            print(f"   ğŸ“ {reason}")
        
        print()
    
    return True

def test_filtering_and_ranking():
    """Test filtering and ranking of multiple listings"""
    print("\nğŸ† Testing Filtering and Ranking")
    print("=" * 40)
    
    analyzer = ContentAnalyzer()
    profile = create_test_profile()
    listings = create_test_listings()
    
    # Analyze all listings
    results = []
    for listing in listings:
        result = analyzer.analyze_listing(listing, profile)
        results.append((listing, result))
    
    # Filter and rank matches
    ranked_matches = analyzer.rank_matches(results)
    
    print(f"ğŸ“Š Analysis Summary:")
    print(f"  Total listings analyzed: {len(listings)}")
    print(f"  Matches found: {len(ranked_matches)}")
    print()
    
    if ranked_matches:
        print("ğŸ† Ranked Matches (Best to Worst):")
        for i, (listing, result) in enumerate(ranked_matches, 1):
            print(f"{i}. {listing.title}")
            print(f"   ğŸ’° {listing.price:,} ILS | ğŸ  {listing.rooms} rooms")
            print(f"   ğŸ¯ Score: {result.score:.1f} | Confidence: {result.confidence.value}")
            print(f"   ğŸ“ {listing.location}")
            print()
    else:
        print("âŒ No matches found")
    
    return len(ranked_matches) > 0

def test_duplicate_detection():
    """Test content hashing for duplicate detection"""
    print("\nğŸ”„ Testing Duplicate Detection")
    print("=" * 40)
    
    analyzer = ContentAnalyzer()
    
    # Create original and duplicate listings
    original = ScrapedListing(
        listing_id="original_1",
        title="×“×™×¨×ª 2 ×—×“×¨×™× ×‘×“×™×–× ×’×•×£",
        price=5800,
        rooms=2.0,
        location="×“×™×–× ×’×•×£ 45, ×ª×œ ××‘×™×‘",
        description="×“×™×¨×” ×™×¤×” ×‘××¨×›×– ×”×¢×™×¨"
    )
    
    # Near duplicate (slight title change)
    near_duplicate = ScrapedListing(
        listing_id="duplicate_1",
        title="×“×™×¨×ª 2 ×—×“×¨×™× ×‘×“×™×–× ×’×•×£ - ××©×•×¤×¦×ª", # Slightly different title
        price=5800,
        rooms=2.0,
        location="×“×™×–× ×’×•×£ 45, ×ª×œ ××‘×™×‘",
        description="×“×™×¨×” ×™×¤×” ×‘××¨×›×– ×”×¢×™×¨"
    )
    
    # Different listing
    different = ScrapedListing(
        listing_id="different_1",
        title="×¡×˜×•×“×™×• ×‘×¨×•×˜×©×™×œ×“",
        price=4200,
        rooms=1.0,
        location="×¨×•×˜×©×™×œ×“ 88, ×ª×œ ××‘×™×‘",
        description="×¡×˜×•×“×™×• ×—×“×©"
    )
    
    original_hash = analyzer.generate_content_hash(original)
    duplicate_hash = analyzer.generate_content_hash(near_duplicate)
    different_hash = analyzer.generate_content_hash(different)
    
    print(f"Original listing: {original.title}")
    print(f"Hash: {original_hash[:16]}...")
    print()
    
    print(f"Near duplicate: {near_duplicate.title}")
    print(f"Hash: {duplicate_hash[:16]}...")
    print(f"Same as original: {'âœ… Yes' if original_hash == duplicate_hash else 'âŒ No'}")
    print()
    
    print(f"Different listing: {different.title}")
    print(f"Hash: {different_hash[:16]}...")
    print(f"Same as original: {'âœ… Yes' if original_hash == different_hash else 'âŒ No'}")
    print()
    
    return True

def test_edge_cases():
    """Test edge cases and error handling"""
    print("\nâš ï¸ Testing Edge Cases")
    print("=" * 40)
    
    analyzer = ContentAnalyzer()
    profile = create_test_profile()
    
    # Test with missing data
    incomplete_listing = ScrapedListing(
        listing_id="incomplete_1",
        title="×“×™×¨×” ×œ×œ× ×¤×¨×˜×™×",
        price=None,  # Missing price
        rooms=None,  # Missing rooms
        location="",  # Empty location
        description=""  # Empty description
    )
    
    print("Testing incomplete listing...")
    result = analyzer.analyze_listing(incomplete_listing, profile)
    print(f"âœ… Handled missing data: match={result.is_match}, score={result.score:.1f}")
    
    # Test with empty profile
    empty_profile = {}
    result2 = analyzer.analyze_listing(create_test_listings()[0], empty_profile)
    print(f"âœ… Handled empty profile: match={result2.is_match}, score={result2.score:.1f}")
    
    # Test text normalization edge cases
    weird_text = "×“×™×¨×”!!! ×¢×    ×¨×•×•×—×™× @@@@ ×•×¡×™×× ×™× #$%^"
    normalized = analyzer.normalize_text(weird_text)
    print(f"âœ… Normalized weird text: '{weird_text}' â†’ '{normalized}'")
    
    return True

def main():
    """Run all content analysis tests"""
    print("ğŸ”¬ RealtyScanner Agent - Content Analysis Test")
    print("=" * 60)
    
    try:
        # Test 1: Text normalization
        success1 = test_text_normalization()
        
        # Test 2: Individual analysis
        success2 = test_individual_analysis()
        
        # Test 3: Filtering and ranking
        success3 = test_filtering_and_ranking()
        
        # Test 4: Duplicate detection
        success4 = test_duplicate_detection()
        
        # Test 5: Edge cases
        success5 = test_edge_cases()
        
        # Summary
        print("=" * 60)
        total_tests = 5
        passed_tests = sum([success1, success2, success3, success4, success5])
        
        if passed_tests == total_tests:
            print("ğŸ‰ All content analysis tests completed successfully!")
            print(f"âœ… {passed_tests}/{total_tests} test categories passed")
            print("\nâœ… Epic 2.2: Content Analysis & Filtering Logic - COMPLETE")
            print("\nThe content analysis system can:")
            print("- Normalize Hebrew and English text")
            print("- Match locations with aliases")
            print("- Filter by price and room criteria")
            print("- Score and rank matches by relevance")
            print("- Generate content hashes for duplicate detection")
            print("- Handle edge cases and missing data")
            print("\nğŸš€ Ready for Epic 2.3: Notification Dispatcher Integration")
            return True
        else:
            print(f"âš ï¸ Some tests had issues ({passed_tests}/{total_tests} passed)")
            return False
            
    except Exception as e:
        print(f"âŒ Content analysis test failed with error: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
